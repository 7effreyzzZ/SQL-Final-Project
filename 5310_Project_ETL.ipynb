{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88241454",
   "metadata": {},
   "source": [
    "# ETL Plan and Reasons\n",
    "To manage the complexity and redundancy of the 6 original datasets, we applied an ETL process to transform them into 16 normalized tables adhering to the principles of 3NF. \n",
    "\n",
    "1. The ALL_Store dataset was split into the store and operating_costs tables to separate static store details (e.g., name, address) from dynamic financial data (e.g., expenses, dates). This normalization avoids redundancy, ensures data consistency, and supports efficient analysis by linking operational costs to stores via store_id.\n",
    "\n",
    "2. The ALL_Products dataset was split into product, category, vendor, and product_vendor tables to normalize the data. The product table manages product-specific details like name, price, and description, while the category table organizes products by their categories for easier classification. The vendor table stores unique vendor details like names and contact information, and the product_vendor table captures the many-to-many relationship between products and vendors, including supply frequency. \n",
    "\n",
    "3. The All_Employee dataset was split into employee, employee_detail, and management tables. The employee table stores core employee details like name, position, ensuring each employee is uniquely identified. The employee_detail table focuses on job-specific attributes like salary, shift type, avoiding redundancy across records. The management table tracks hierarchical relationships, linking managers to their employees and associated stores, enabling clear representation of reporting structures and responsibilities.\n",
    "\n",
    "4. The ALL_Product_stock dataset was transformed into the product_stock table to track inventory levels, linking products and stores via product_id and store_id for efficient stock management and analysis.\n",
    "\n",
    "5. The ALL_Customer dataset was split into customer and loyalty_program tables to separate static customer information from dynamic program data. The customer table captures core details like name, contact information, and address, ensuring a single source of truth for customer profiles. The loyalty_program table focuses on program-specific data like membership start date, total points, and tier, which are tied to individual customers via customer_id. \n",
    "\n",
    "6. The All_Transactions dataset was split into customer_transactions, transaction_details, payment, and customer_feedback tables to normalize and organize transaction data. Customer_transactions records high-level details like customers, stores, and dates, while transaction_details tracks purchased products and quantities. Payment consolidates financial data like amounts and methods, and customer_feedback isolates ratings and comments for sentiment analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45e5eb6",
   "metadata": {},
   "source": [
    "# Create the Table Schemas for our relational database design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4131b043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tables created successfully!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Database connection configuration\n",
    "db_config = {\n",
    "    \"dbname\": \"5310_data\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"123\",\n",
    "    \"host\": \"localhost\",  \n",
    "    \"port\": \"5432\"        \n",
    "}\n",
    "\n",
    "# SQL statements\n",
    "create_tables_sql = \"\"\"\n",
    "-- 1. Employee Table\n",
    "CREATE TABLE IF NOT EXISTS employee (\n",
    "    employee_id VARCHAR(50) PRIMARY KEY,\n",
    "    first_name VARCHAR(25) NOT NULL,\n",
    "    last_name VARCHAR(25) NOT NULL,\n",
    "    position VARCHAR(50) NOT NULL,\n",
    "    date_hired DATE NOT NULL,\n",
    "    address VARCHAR(255),\n",
    "    contact_number VARCHAR(15),\n",
    "    employment_status VARCHAR(20) NOT NULL\n",
    ");\n",
    "\n",
    "-- 2. Store Table\n",
    "CREATE TABLE IF NOT EXISTS store (\n",
    "    store_id VARCHAR(50) PRIMARY KEY,\n",
    "    store_name VARCHAR(100),\n",
    "    address VARCHAR(255),\n",
    "    city VARCHAR(50),\n",
    "    state VARCHAR(50),\n",
    "    phone_number VARCHAR(15),\n",
    "    email VARCHAR(100)\n",
    ");\n",
    "\n",
    "-- 3. Employee_Detail Table\n",
    "CREATE TABLE IF NOT EXISTS employee_detail (\n",
    "    employee_id VARCHAR(50) PRIMARY KEY,\n",
    "    salary DECIMAL(10, 2),\n",
    "    shift_type VARCHAR(20),\n",
    "    store_id VARCHAR(50),\n",
    "    FOREIGN KEY (employee_id) REFERENCES employee(employee_id) ON DELETE CASCADE,\n",
    "    FOREIGN KEY (store_id) REFERENCES store(store_id) ON DELETE SET NULL\n",
    ");\n",
    "\n",
    "-- 4. Management Table\n",
    "CREATE TABLE IF NOT EXISTS management (\n",
    "    manager_id VARCHAR(10),\n",
    "    managing VARCHAR(10),\n",
    "    store_id VARCHAR(50),\n",
    "    PRIMARY KEY (manager_id, managing),\n",
    "    FOREIGN KEY (store_id) REFERENCES store(store_id)\n",
    ");\n",
    "\n",
    "-- 5. Customer Table\n",
    "CREATE TABLE IF NOT EXISTS customer (\n",
    "    customer_id VARCHAR(50) PRIMARY KEY,\n",
    "    first_name VARCHAR(50) NOT NULL,\n",
    "    last_name VARCHAR(50) NOT NULL,\n",
    "    email VARCHAR(100) UNIQUE NOT NULL,\n",
    "    phone_number VARCHAR(15),\n",
    "    date_of_birth DATE,\n",
    "    address VARCHAR(255),\n",
    "    city VARCHAR(100),\n",
    "    state VARCHAR(50),\n",
    "    zip_code VARCHAR(10)\n",
    ");\n",
    "\n",
    "-- 6. Loyalty_Program Table\n",
    "CREATE TABLE IF NOT EXISTS loyalty_program (\n",
    "    customer_id VARCHAR(50) PRIMARY KEY,\n",
    "    membership_start_date DATE NOT NULL,\n",
    "    total_points INT DEFAULT 0,\n",
    "    membership_tier VARCHAR(50) CHECK (membership_tier IN ('Bronze', 'Silver', 'Gold')),\n",
    "    FOREIGN KEY (customer_id) REFERENCES customer(customer_id) ON DELETE CASCADE\n",
    ");\n",
    "\n",
    "-- 7. Product Table\n",
    "CREATE TABLE IF NOT EXISTS product (\n",
    "    product_id VARCHAR(50) PRIMARY KEY,\n",
    "    category_id VARCHAR(50) NOT NULL,\n",
    "    product_name VARCHAR(250) NOT NULL,\n",
    "    unit_price DECIMAL(10, 2) NOT NULL,\n",
    "    description TEXT\n",
    ");\n",
    "\n",
    "\n",
    "-- 8. Category Table\n",
    "CREATE TABLE IF NOT EXISTS category (\n",
    "    category_id VARCHAR(50) PRIMARY KEY,\n",
    "    category_name VARCHAR(100) NOT NULL,\n",
    "    FOREIGN KEY (category_id) REFERENCES category(category_id) ON DELETE SET NULL\n",
    ");\n",
    "\n",
    "\n",
    "-- 9. Product_Stock Table\n",
    "CREATE TABLE IF NOT EXISTS product_stock (\n",
    "    stock_id VARCHAR(50) PRIMARY KEY,\n",
    "    store_id VARCHAR(50) NOT NULL,\n",
    "    product_id VARCHAR(50) NOT NULL,\n",
    "    quantity INT,\n",
    "    last_stocked_date DATE,\n",
    "    FOREIGN KEY (store_id) REFERENCES store(store_id) ON DELETE CASCADE,\n",
    "    FOREIGN KEY (product_id) REFERENCES product(product_id) ON DELETE CASCADE\n",
    ");\n",
    "\n",
    "-- 10. Vendor Table\n",
    "CREATE TABLE IF NOT EXISTS vendor (\n",
    "    vendor_id VARCHAR(50) PRIMARY KEY,\n",
    "    name VARCHAR(100),\n",
    "    contact_info VARCHAR(255),\n",
    "    address VARCHAR(255)\n",
    ");\n",
    "\n",
    "-- 11. Product_Vendor Table\n",
    "CREATE TABLE IF NOT EXISTS product_vendor (\n",
    "    product_id VARCHAR(50) NOT NULL,\n",
    "    vendor_id VARCHAR(50) NOT NULL,\n",
    "    supply_frequency VARCHAR(50),\n",
    "    PRIMARY KEY (product_id, vendor_id),\n",
    "    FOREIGN KEY (product_id) REFERENCES product(product_id) ON DELETE CASCADE,\n",
    "    FOREIGN KEY (vendor_id) REFERENCES vendor(vendor_id) ON DELETE CASCADE\n",
    ");\n",
    "\n",
    "-- 12. Operating_Costs Table\n",
    "CREATE TABLE IF NOT EXISTS operating_costs (\n",
    "    operating_id VARCHAR(50) PRIMARY KEY,\n",
    "    store_id VARCHAR(50) NOT NULL,\n",
    "    expense_date DATE,\n",
    "    expense_category VARCHAR(50),\n",
    "    amount DECIMAL(10, 2) NOT NULL,\n",
    "    FOREIGN KEY (store_id) REFERENCES store(store_id) ON DELETE CASCADE\n",
    ");\n",
    "\n",
    "-- 13. Customer_Transactions Table\n",
    "CREATE TABLE IF NOT EXISTS customer_transactions (\n",
    "    transaction_id VARCHAR(50) PRIMARY KEY,\n",
    "    store_id VARCHAR(5) NOT NULL,\n",
    "    customer_id VARCHAR(50),\n",
    "    transaction_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    FOREIGN KEY (store_id) REFERENCES store(store_id) ON DELETE CASCADE,\n",
    "    FOREIGN KEY (customer_id) REFERENCES customer(customer_id) ON DELETE SET NULL\n",
    ");\n",
    "\n",
    "-- 14. Payment Table\n",
    "CREATE TABLE IF NOT EXISTS payment (\n",
    "    transaction_id VARCHAR(50) NOT NULL,\n",
    "    payment_method VARCHAR(50) NOT NULL,\n",
    "    payment_amount DECIMAL(10, 2) NOT NULL,\n",
    "    payment_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    PRIMARY KEY (transaction_id),\n",
    "    FOREIGN KEY (transaction_id) REFERENCES customer_transactions(transaction_id) ON DELETE CASCADE\n",
    ");\n",
    "\n",
    "-- 15. Transaction_Details Table\n",
    "CREATE TABLE IF NOT EXISTS transaction_details (\n",
    "    transaction_id VARCHAR(50) NOT NULL,\n",
    "    product_id VARCHAR(50) NOT NULL,\n",
    "    quantity INT,\n",
    "    PRIMARY KEY (transaction_id, product_id),\n",
    "    FOREIGN KEY (transaction_id) REFERENCES customer_transactions(transaction_id) ON DELETE CASCADE,\n",
    "    FOREIGN KEY (product_id) REFERENCES product(product_id) ON DELETE SET NULL\n",
    ");\n",
    "\n",
    "-- 16. Customer_Feedback Table\n",
    "CREATE TABLE IF NOT EXISTS customer_feedback (\n",
    "    customer_id VARCHAR(50) NOT NULL,\n",
    "    transaction_id VARCHAR(50),\n",
    "    feedback TEXT,\n",
    "    rating DECIMAL(3, 2) CHECK (rating BETWEEN 0 AND 5),\n",
    "    PRIMARY KEY (transaction_id, customer_id),\n",
    "    FOREIGN KEY (customer_id) REFERENCES customer(customer_id) ON DELETE CASCADE,\n",
    "    FOREIGN KEY (transaction_id) REFERENCES customer_transactions(transaction_id) ON DELETE SET NULL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Connect to the database and create tables\n",
    "try:\n",
    "    conn = psycopg2.connect(**db_config)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(create_tables_sql)\n",
    "    conn.commit()\n",
    "    print(\"All tables created successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "finally:\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if conn:\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb1e7f6",
   "metadata": {},
   "source": [
    "# Extract, Transform and Load (ETL)\n",
    "#### Data Manipulation 1 - Store Table\n",
    "Create store and operating_costs two data frames includes most of the values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "642af444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Pass the connection string to a variable, conn_url\n",
    "con_url = 'postgresql://postgres:123@localhost/5310_data'\n",
    "engine = create_engine(con_url)\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53022e3e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('/Users/godu/Desktop/鼠🐭/哥大/5310 SQL/Group_project/All_Store.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91c9432a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  expense_date      store_name                       address      city state  \\\n",
      "0    2022/4/29  ABC FoodMart B        456 Elm Street, Queens  New York    NY   \n",
      "1     2023/5/3  ABC FoodMart B        456 Elm Street, Queens  New York    NY   \n",
      "2    2023/8/19  ABC FoodMart C      101 Cedar Road, Brooklyn  New York    NY   \n",
      "3    2024/7/25  ABC FoodMart A      123 Maple Avenue, Queens  New York    NY   \n",
      "4    2022/1/21  ABC FoodMart D  789 Pine Boulevard, Brooklyn  New York    NY   \n",
      "5   2023/11/12  ABC FoodMart C      101 Cedar Road, Brooklyn  New York    NY   \n",
      "6    2024/6/15  ABC FoodMart C      101 Cedar Road, Brooklyn  New York    NY   \n",
      "7   2023/10/27  ABC FoodMart D  789 Pine Boulevard, Brooklyn  New York    NY   \n",
      "8    2024/7/11  ABC FoodMart A      123 Maple Avenue, Queens  New York    NY   \n",
      "9    2024/7/22  ABC FoodMart B        456 Elm Street, Queens  New York    NY   \n",
      "\n",
      "   phone_number                   email expense_category   amount  store_id  \n",
      "0  881-729-5270  ABCFoodMartB@gmail.com             Rent  3729.39         2  \n",
      "1  881-729-5270  ABCFoodMartB@gmail.com      Maintenance  2210.33         2  \n",
      "2  881-751-7981  ABCFoodMartC@gmail.com             Rent  3208.38         3  \n",
      "3  664-713-7151  ABCFoodMartA@gmail.com      Maintenance  1135.39         1  \n",
      "4  881-729-5272  ABCFoodMartD@gmail.com        Utilities   904.52         4  \n",
      "5  881-751-7981  ABCFoodMartC@gmail.com        Utilities   684.24         3  \n",
      "6  881-751-7981  ABCFoodMartC@gmail.com      Maintenance  1970.85         3  \n",
      "7  881-729-5272  ABCFoodMartD@gmail.com      Advertising  1206.55         4  \n",
      "8  664-713-7151  ABCFoodMartA@gmail.com        Utilities   856.41         1  \n",
      "9  881-729-5270  ABCFoodMartB@gmail.com      Maintenance  1080.97         2  \n"
     ]
    }
   ],
   "source": [
    "# Create a mapping of store_name to store_id\n",
    "# Create a DataFrame with unique store names and their store_id\n",
    "store_df = pd.DataFrame(df1.store_name.unique(), columns=['store_name'])\n",
    "\n",
    "letter_mapping = {chr(i): i - 64 for i in range(65, 91)}\n",
    "\n",
    "# Extract the last letter from store_name and map it to the corresponding number\n",
    "store_df['store_id'] = store_df['store_name'].str.extract(r'([A-Z])$')[0].map(letter_mapping)\n",
    "\n",
    "# Merge the store_id back into df1\n",
    "df1 = df1.merge(store_df, on='store_name', how='left')\n",
    "\n",
    "# Display the updated df1\n",
    "print(df1.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3689970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   store_id      store_name                       address      city state  \\\n",
      "0         1  ABC FoodMart A      123 Maple Avenue, Queens  New York    NY   \n",
      "1         2  ABC FoodMart B        456 Elm Street, Queens  New York    NY   \n",
      "2         3  ABC FoodMart C      101 Cedar Road, Brooklyn  New York    NY   \n",
      "3         4  ABC FoodMart D  789 Pine Boulevard, Brooklyn  New York    NY   \n",
      "4         5  ABC FoodMart E        202 Oak Lane, Brooklyn  New York    NY   \n",
      "\n",
      "   phone_number                   email  \n",
      "0  664-713-7151  ABCFoodMartA@gmail.com  \n",
      "1  881-729-5270  ABCFoodMartB@gmail.com  \n",
      "2  881-751-7981  ABCFoodMartC@gmail.com  \n",
      "3  881-729-5272  ABCFoodMartD@gmail.com  \n",
      "4  881-729-5273  ABCFoodMartE@gmail.com  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Insert to store table\n",
    "# Ensure 'store_id', 'store_name', 'address', 'city', 'state', 'phone_number', 'email' columns exist\n",
    "# Drop duplicates based on 'store_id' to get unique store information\n",
    "tem_store_df = df1[['store_id', 'store_name', 'address', 'city', 'state', 'phone_number', 'email']].drop_duplicates()\n",
    "\n",
    "# Sort by 'store_id'\n",
    "tem_store_df = tem_store_df.sort_values(by='store_id').reset_index(drop=True)\n",
    "\n",
    "# Display the result\n",
    "print(tem_store_df.head())\n",
    "tem_store_df.to_sql(name='store', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c4709e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  operating_id  store_id expense_date expense_category   amount\n",
      "0           O1         2    2022/4/29             Rent  3729.39\n",
      "1           O2         2     2023/5/3      Maintenance  2210.33\n",
      "2           O3         3    2023/8/19             Rent  3208.38\n",
      "3           O4         1    2024/7/25      Maintenance  1135.39\n",
      "4           O5         4    2022/1/21        Utilities   904.52\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create opr_cost_df with the selected columns\n",
    "opr_cost_df = df1[['store_id', 'expense_date', 'expense_category', 'amount']]\n",
    "\n",
    "# Add Operating_id column with values starting from O1\n",
    "opr_cost_df.insert(0, 'operating_id', ['O' + str(i) for i in range(1, len(opr_cost_df) + 1)])\n",
    "\n",
    "# Display the result\n",
    "print(opr_cost_df.head())\n",
    "opr_cost_df.to_sql(name='operating_costs', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb34154",
   "metadata": {},
   "source": [
    "#### Data Manipulation 2 - ALL_Product Table\n",
    "#### Create product, category, vendor, product_vendor four data frames includes most of the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ad6fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('/Users/godu/Desktop/鼠🐭/哥大/5310 SQL/Group_project/ALL_Products.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f90deda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        product_name  \\\n",
      "0  David’s Cookies Mile High Peanut Butter Cake, ...   \n",
      "1  The Cake Bake Shop 8\" Round Carrot Cake (16-22...   \n",
      "2  St Michel Madeleine, Classic French Sponge Cak...   \n",
      "3  David's Cookies Butter Pecan Meltaways 32 oz, ...   \n",
      "4  David’s Cookies Premier Chocolate Cake, 7.2 lb...   \n",
      "\n",
      "                                         description unit_price  \\\n",
      "0  A cake the dessert epicure will die for!Our To...      56.99   \n",
      "1  Due to the perishable nature of this item, ord...     159.99   \n",
      "2  Moist and buttery sponge cakes with the tradit...      44.99   \n",
      "3  These delectable butter pecan meltaways are th...      39.99   \n",
      "4  A cake the dessert epicure will die for!To the...      59.99   \n",
      "\n",
      "            Category                 vendor_name                contact_info  \\\n",
      "0  Bakery & Desserts  Dairy Delight Distributors    dairydelight@example.com   \n",
      "1  Bakery & Desserts           Global Foods Ltd.     globalfoods@example.com   \n",
      "2  Bakery & Desserts           Refreshment Depot     refreshment@example.com   \n",
      "3  Bakery & Desserts         Fresh Catch Seafood      freshcatch@example.com   \n",
      "4  Bakery & Desserts        Organic Grocers Ltd.  organicgrocers@example.com   \n",
      "\n",
      "                            address supply_frequency product_id category_id  \\\n",
      "0         456 Milk Rd, Brooklyn, NY        Quarterly         P1         CE1   \n",
      "1     123 Supply Blvd, Brooklyn, NY          Monthly         P2         CE1   \n",
      "2        456 Chill Ln, Brooklyn, NY        Quarterly         P3         CE1   \n",
      "3       789 Fisherman Rd, Bronx, NY        On-Demand         P4         CE1   \n",
      "4  345 Fresh Ave, Staten Island, NY            Daily         P5         CE1   \n",
      "\n",
      "  vendor_id  \n",
      "0        V1  \n",
      "1        V2  \n",
      "2        V3  \n",
      "3        V4  \n",
      "4        V5  \n"
     ]
    }
   ],
   "source": [
    "#Insert into product table\n",
    "# Add `product_id` column based on unique `product_name`\n",
    "product_mapping = {name: f'P{i+1}' for i, name in enumerate(df2['product_name'].unique())}\n",
    "df2['product_id'] = df2['product_name'].map(product_mapping)\n",
    "\n",
    "#  Add `category_id` column based on unique `Category`\n",
    "category_mapping = {name: f'CE{i+1}' for i, name in enumerate(df2['Category'].unique())}\n",
    "df2['category_id'] = df2['Category'].map(category_mapping)\n",
    "\n",
    "# Add `vendor_id` column based on unique `vendor_name`\n",
    "vendor_mapping = {name: f'V{i+1}' for i, name in enumerate(df2['vendor_name'].unique())}\n",
    "df2['vendor_id'] = df2['vendor_name'].map(vendor_mapping)\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "print(df2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4179f526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "483"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the `unit_price` column\n",
    "df2['unit_price'] = df2['unit_price'].replace({',': ''}, regex=True).astype(float)\n",
    "\n",
    "\n",
    "# Insert cleaned data into the product database\n",
    "df2[['product_id', 'category_id', 'product_name', 'unit_price', 'description']].to_sql(\n",
    "    'product', engine, if_exists='append', index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5c0e8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Insert into category table\n",
    "category_df = df2[['category_id', 'Category']].drop_duplicates().rename(columns={'Category': 'category_name'})\n",
    "\n",
    "category_df.to_sql('category', engine, if_exists='append', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ee40ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Insert into vendor table\n",
    "vendor_df = df2[['vendor_id', 'vendor_name',\"contact_info\",\"address\"]].drop_duplicates().rename(columns={'vendor_name': 'name'})\n",
    "\n",
    "vendor_df.to_sql('vendor', engine, if_exists='append', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7672c3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "483"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Insert into product_vendor table\n",
    "product_vendor = df2[['product_id','vendor_id', \"supply_frequency\"]].drop_duplicates()\n",
    "\n",
    "product_vendor.to_sql('product_vendor', engine, if_exists='append', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a844915",
   "metadata": {},
   "source": [
    "#### Data Manipulation 3 - ALL_Employee Table\n",
    "#### Create employee, employee_detail, management three data frames includes most of the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2e560f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('/Users/godu/Desktop/鼠🐭/哥大/5310 SQL/Group_project/All_Employee.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b77883d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  employee_id first_name last_name       position date_hired  \\\n",
      "0          E1    Brandon    Martin  Store Manager   2013/8/7   \n",
      "1          E2     Monica     Garza  Store Manager  2016/12/5   \n",
      "2          E3       Gina    Bailey  Store Manager  2013/10/1   \n",
      "3          E4    Anthony     Hicks  Store Manager  2015/3/10   \n",
      "4          E5      Renee     Chang  Store Manager   2019/2/5   \n",
      "\n",
      "                                             address contact_number  \\\n",
      "0                  Unit 8989 Box 4169\\n DPO AA 59272  (312)555-0184   \n",
      "1          36921 Benson Pike\\n Amandamouth, PA 69214  (570)841-6515   \n",
      "2       35455 Sarah Lakes\\n Griffinchester, UT 68291  (810)706-7631   \n",
      "3  58959 William Knoll Suite 738\\n Kathleenshire,...  (608)818-6555   \n",
      "4  869 Michael Brooks Apt. 369\\n East Michaelfort...  (617)798-5530   \n",
      "\n",
      "  employment_status  \n",
      "0          On Leave  \n",
      "1            Active  \n",
      "2            Active  \n",
      "3            Active  \n",
      "4            Active  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Insert into employee table\n",
    "# Ensure 'employee_id', 'first_name', 'last_name', 'position', 'date_hired', 'address', 'contact_number', 'employment_status' columns exist\n",
    "# Drop duplicates based on 'employee_id' to get unique employee information\n",
    "\n",
    "temp_employee_df = df3[['employee_id', 'first_name', 'last_name', 'position', \n",
    "                        'date_hired', 'address', 'contact_number', 'employment_status']].drop_duplicates()\n",
    "\n",
    "# Extract the numeric part of 'employee_id' for sorting\n",
    "temp_employee_df['id_number'] = temp_employee_df['employee_id'].str.extract('(\\d+)', expand=False).astype(int)\n",
    "\n",
    "# Sort by the extracted numeric part of 'employee_id'\n",
    "temp_employee_df = temp_employee_df.sort_values(by='id_number').drop(columns=['id_number']).reset_index(drop=True)\n",
    "\n",
    "# Display the first few rows to verify the results\n",
    "print(temp_employee_df.head())\n",
    "\n",
    "\n",
    "temp_employee_df.to_sql(name='employee', con=engine, if_exists='append', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5395c284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  employee_id  salary shift_type  store_id\n",
      "0          E1   89000    Daytime         1\n",
      "1          E2   89000    Daytime         2\n",
      "2          E3   89000    Daytime         3\n",
      "3          E4   89000    Daytime         4\n",
      "4          E5   89000    Daytime         5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#insert into employee_detail\n",
    "# Create a mapping between store_name and store_id from df1\n",
    "store_mapping = df1.set_index('store_name')['store_id'].to_dict()\n",
    "\n",
    "# Ensure 'employee_id', 'salary', 'shift_type', and 'store_id' columns exist\n",
    "# Map store_name to store_id using the mapping\n",
    "temp_employee_detail_df = df3[['employee_id', 'salary', 'shift_type', 'store_name']].drop_duplicates()\n",
    "temp_employee_detail_df['store_id'] = temp_employee_detail_df['store_name'].map(store_mapping)\n",
    "\n",
    "# Remove the original 'store_name' column as it is no longer needed\n",
    "temp_employee_detail_df = temp_employee_detail_df.drop(columns=['store_name'])\n",
    "\n",
    "# Extract numeric part of employee_id for sorting\n",
    "temp_employee_detail_df['employee_id_numeric'] = temp_employee_detail_df['employee_id'].str.extract(r'(\\d+)$').astype(int)\n",
    "\n",
    "# Sort by the numeric part of employee_id\n",
    "temp_employee_detail_df = temp_employee_detail_df.sort_values(by='employee_id_numeric').reset_index(drop=True)\n",
    "\n",
    "temp_employee_detail_df = temp_employee_detail_df.drop(columns=['employee_id_numeric'])\n",
    "\n",
    "print(temp_employee_detail_df.head())\n",
    "\n",
    "temp_employee_detail_df.to_sql(name='employee_detail', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55dca196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  manager_id managing store_id\n",
      "0         E1       E6        1\n",
      "1         E1      E11        1\n",
      "2         E1      E16        1\n",
      "3         E1      E21        1\n",
      "4         E2       E7        2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#insert into management table\n",
    "# Create a mapping between store_name and store_id from df1\n",
    "store_mapping = df1.set_index('store_name')['store_id'].to_dict()\n",
    "\n",
    "# Initialize an empty DataFrame for the management table\n",
    "manager_table = pd.DataFrame(columns=['manager_id', 'managing', 'store_id'])\n",
    "\n",
    "# Iterate through the rows of the DataFrame\n",
    "for _, row in df3.iterrows():\n",
    "    # Check if 'management' is a valid string before splitting\n",
    "    if isinstance(row['management'], str):\n",
    "        manager_ids = row['management'].split(',')\n",
    "        for manager_id in manager_ids:\n",
    "            # Append data to the manager_table DataFrame\n",
    "            manager_table = pd.concat([manager_table, pd.DataFrame({\n",
    "                'manager_id': [row['employee_id']],  # Now `employee_id` is the manager\n",
    "                'managing': [manager_id],           # `manager_id` becomes the managed employee\n",
    "                'store_id': [store_mapping.get(row['store_name'], None)]  # Map store_name to store_id\n",
    "            })], ignore_index=True)\n",
    "\n",
    "# Drop rows with missing store_id\n",
    "manager_table = manager_table.dropna(subset=['store_id'])\n",
    "\n",
    "# Display the first few rows of the manager_table\n",
    "print(manager_table.head())\n",
    "manager_table.to_sql(name='management', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc6237",
   "metadata": {},
   "source": [
    "#### Data Manipulation 4 - ALL_Product_stock Table\n",
    "#### Create product_stock data frames includes most of the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dfa71b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert into product_stock\n",
    "df4 = pd.read_csv('/Users/godu/Desktop/鼠🐭/哥大/5310 SQL/Group_project/ALL_Product_stock.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "683bbb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  stock_id  store_id product_id  quantity last_stocked_date\n",
      "0       S1         1         P1       247         5/22/2023\n",
      "1       S2         2         P1       920          7/8/2023\n",
      "2       S3         3         P1       696         8/30/2023\n",
      "3       S4         4         P1       772        11/12/2024\n",
      "4       S5         5         P1       980         7/16/2023\n"
     ]
    }
   ],
   "source": [
    "# Map store_name to store_id using df1\n",
    "store_mapping = df1.set_index('store_name')['store_id'].to_dict()\n",
    "\n",
    "# Map product_name to product_id using df3\n",
    "product_mapping = df2.set_index('product_name')['product_id'].to_dict()\n",
    "\n",
    "\n",
    "# Create stock_id with 'S1', 'S2', 'S3' format\n",
    "df4.insert(0, 'stock_id', [f'S{i+1}' for i in range(len(df4))])\n",
    "\n",
    "# Map store_name to store_id\n",
    "df4['store_id'] = df4['store_name'].map(store_mapping)\n",
    "\n",
    "# Map product_name to product_id\n",
    "df4['product_id'] = df4['product_name'].map(product_mapping)\n",
    "\n",
    "df4 = df4[['stock_id', 'store_id', 'product_id', 'quantity', 'last_stocked_date']]\n",
    "\n",
    "print(df4.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "902efc1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "575"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.to_sql(name='product_stock', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f793c2",
   "metadata": {},
   "source": [
    "#### Data Manipulation 5 - ALL_Customer Table\n",
    "#### Create customer, loyalty_program two data frames includes most of the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d993bc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  customer_id first_name last_name    phone_number  \\\n",
      "0          C1      Uriah   Bridges  (401) 552-4059   \n",
      "1          C2      Paula     Small  (688) 210-1295   \n",
      "2          C3     Edward      Buck  (434) 593-6233   \n",
      "3          C4    Michael   Riordan  (517) 270-8979   \n",
      "4          C5    Jasmine     Onque  (214) 940-9676   \n",
      "\n",
      "                         email date_of_birth           address         city  \\\n",
      "0    uriah.bridges@example.com     1977/10/1  8413 Madison Ave     New York   \n",
      "1      paula.small@example.com      1969/7/5     5064 Broadway  Jersey City   \n",
      "2      edward.buck@example.com     2000/3/26      8055 Main St     New York   \n",
      "3  michael.riordan@example.com      1981/2/8      2880 Main St     New York   \n",
      "4    jasmine.onque@example.com     1961/4/10     9447 Broadway     New York   \n",
      "\n",
      "        state  zip_code  \n",
      "0    New York     10005  \n",
      "1  New Jersey      7004  \n",
      "2    New York     10004  \n",
      "3    New York     10003  \n",
      "4    New York     10005  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "753"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert into customer\n",
    "# Read data\n",
    "df5 = pd.read_csv('/Users/godu/Desktop/鼠🐭/哥大/5310 SQL/Group_project/ALL_Customer.csv')\n",
    "\n",
    "# Define the columns to include in the customer table (customer_id as the first column)\n",
    "customer_columns = ['customer_id', 'first_name', 'last_name', 'phone_number', 'email', 'date_of_birth', 'address', 'city', 'state', 'zip_code']\n",
    "\n",
    "# Rename 'zipcode' to 'zip_code' in the DataFrame\n",
    "df5 = df5.rename(columns={'zipcode': 'zip_code'})\n",
    "\n",
    "# Add the customer_id column with 'C1', 'C2', 'C3' format\n",
    "df5.insert(0, 'customer_id', [f'C{i+1}' for i in range(len(df5))])\n",
    "\n",
    "# Extract only the relevant columns in the desired order\n",
    "customer_df = df5[customer_columns]\n",
    "\n",
    "# Display the first few rows of the resulting DataFrame\n",
    "print(customer_df.head())\n",
    "\n",
    "# Insert the data into the customer table in the database\n",
    "customer_df.to_sql(name='customer', con=engine, if_exists='append', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b3e2285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  customer_id membership_startdate  total_points membership_tier\n",
      "0          C1           2006/12/17          1251          Silver\n",
      "1          C2           2007/11/20           523          Bronze\n",
      "2          C3            2006/3/13          1702          Silver\n",
      "3          C4            2019/9/28          6521            Gold\n",
      "4          C5            2014/5/12           686          Bronze\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qp/swc62db92qj289rw105xwzf40000gn/T/ipykernel_63208/1855516383.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  loyalty_program_df.loc[:, 'customer_id'] = df5['email'].map(customer_mapping)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "753"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert into loyalty_program\n",
    "# Define the columns for the loyalty program table\n",
    "loyalty_program_columns = ['membership_startdate', 'total_points', 'membership_tier']\n",
    "loyalty_program_df = df5[loyalty_program_columns]\n",
    "\n",
    "# Map customer_id from the customer_df using customer_id as the unique key\n",
    "# Ensure customer_df contains customer_id and the required columns for mapping\n",
    "customer_mapping = customer_df.set_index('email')['customer_id'].to_dict()  # Use email as the unique key for mapping\n",
    "\n",
    "# Add the customer_id column to the loyalty_program_df using the mapping\n",
    "loyalty_program_df.loc[:, 'customer_id'] = df5['email'].map(customer_mapping)\n",
    "\n",
    "# Drop rows with missing customer_id (if email does not match)\n",
    "loyalty_program_df = loyalty_program_df.dropna(subset=['customer_id'])\n",
    "\n",
    "# Reorder columns to match the loyalty_program table structure\n",
    "loyalty_program_df = loyalty_program_df[['customer_id', 'membership_startdate', 'total_points', 'membership_tier']]\n",
    "\n",
    "# Display the first few rows of the resulting DataFrame\n",
    "print(loyalty_program_df.head())\n",
    "\n",
    "# Insert the data into the loyalty_program table in the database\n",
    "loyalty_program_df.to_sql(name='loyalty_program', con=engine, if_exists='append', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f519ae47",
   "metadata": {},
   "source": [
    "#### Data Manipulation 6 - ALL_Transaction Table\n",
    "#### Create customer_transaction, Transaction_Details,payment,customer_feedback four data frames includes most of the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d7970a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert into customer_transactions\n",
    "# Read data\n",
    "df6 = pd.read_csv('/Users/godu/Desktop/鼠🐭/哥大/5310 SQL/Group_project/All_Transactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6d0eea81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    transaction_id  store_id customer_id transaction_date\n",
      "0        123002578         2       C1010         2023/3/8\n",
      "4        123004074         2        C485        2023/6/11\n",
      "10       123000871         5       C1635         2023/7/7\n",
      "19       123002851         1        C752        2023/1/20\n",
      "27       123003607         3       C1541       2023/11/17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "382"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map store_name to store_id using df1\n",
    "store_mapping = df1.set_index('store_name')['store_id'].to_dict()\n",
    "\n",
    "# Map phone_number to customer_id using df5\n",
    "customer_mapping = df5.set_index('phone_number')['customer_id'].to_dict()\n",
    "\n",
    "\n",
    "# Map store_name to store_id\n",
    "df6['store_id'] = df6['store_name'].map(store_mapping)\n",
    "\n",
    "# Map phone_number to customer_id\n",
    "df6['customer_id'] = df6['phone_number'].map(customer_mapping)\n",
    "\n",
    "# Drop rows with missing store_id or customer_id\n",
    "df6 = df6.dropna(subset=['store_id', 'customer_id'])\n",
    "\n",
    "df6 = df6.drop_duplicates(subset=['transaction_id'])\n",
    "\n",
    "df6_cleaned = df6[['transaction_id', 'store_id', 'customer_id', 'transaction_date']]\n",
    "\n",
    "print(df6_cleaned.head())\n",
    "df6_cleaned.to_sql(name='customer_transactions', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b2852f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    transaction_id product_id  quantity\n",
      "0        123000012       P112         2\n",
      "1        123000012       P125         3\n",
      "2        123000012        P74         2\n",
      "3        123000012       P885         2\n",
      "4        123000012       P911         2\n",
      "5        123000012        P98         2\n",
      "6        123000064      P1276         2\n",
      "7        123000064       P205         2\n",
      "8        123000064       P291         3\n",
      "9        123000064        P42         3\n",
      "10       123000064       P662         1\n",
      "11       123000064       P666         2\n",
      "12       123000121         P1         2\n",
      "13       123000121       P205         2\n",
      "14       123000121       P499         3\n",
      "15       123000121       P510         2\n",
      "16       123000121       P522         2\n",
      "17       123000121       P832         2\n",
      "18       123000121       P907         1\n",
      "19       123000121       P948         3\n"
     ]
    }
   ],
   "source": [
    "# insert into transaction_details \n",
    "df6 = pd.read_csv('/Users/godu/Desktop/鼠🐭/哥大/5310 SQL/Group_project/All_Transactions.csv')\n",
    "\n",
    "# Map product_name to product_id using df2\n",
    "product_mapping = df2.set_index('product_name')['product_id'].to_dict()\n",
    "\n",
    "# Map product_name to product_id in df6\n",
    "df6['product_id'] = df6['product_name'].map(product_mapping)\n",
    "\n",
    "# Drop rows with missing product_id\n",
    "df6 = df6.dropna(subset=['product_id'])\n",
    "\n",
    "# Keep only the necessary columns\n",
    "transaction_details_df = df6[['transaction_id', 'product_id', 'quantity']]\n",
    "\n",
    "# Sort the data by transaction_id and product_id to organize the table\n",
    "transaction_details_df = transaction_details_df.sort_values(by=['transaction_id', 'product_id']).reset_index(drop=True)\n",
    "\n",
    "# Ensure no duplicates in transaction_id and product_id pairs (use for validation, optional)\n",
    "duplicate_check = transaction_details_df.duplicated(subset=['transaction_id', 'product_id'], keep=False)\n",
    "if duplicate_check.any():\n",
    "    print(\"Warning: Duplicates detected in transaction_id and product_id pairs!\")\n",
    "\n",
    "print(transaction_details_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "51b1dd27",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "684"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction_details_df.to_sql(name='transaction_details', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2bdd8eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  customer_id  transaction_id  \\\n",
      "0       C1010       123002578   \n",
      "1        C485       123004074   \n",
      "2       C1635       123000871   \n",
      "3        C752       123002851   \n",
      "4       C1541       123003607   \n",
      "\n",
      "                                           feedback  rating  \n",
      "0             Quick delivery and fantastic service.       4  \n",
      "1                 Very disappointing, poor quality.       1  \n",
      "2           It's okay, not bad but nothing special.       3  \n",
      "3            Excellent product, highly recommended!       4  \n",
      "4  Great quality, very satisfied with the purchase.       5  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "382"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert into customer_feedback\n",
    "# Merge df6 with customer_transactions_df on transaction_id to get customer_id\n",
    "merged_df = pd.merge(df6, df6_cleaned[['transaction_id', 'customer_id']],\n",
    "                     on='transaction_id', how='left')\n",
    "\n",
    "# Select and reorder the necessary columns\n",
    "customer_feedback_df = merged_df[['customer_id', 'transaction_id', 'feedback', 'rating']]\n",
    "\n",
    "# Remove duplicate transaction_id, keeping only the first occurrence\n",
    "customer_feedback_df = customer_feedback_df.drop_duplicates(subset=['transaction_id'], keep='first').reset_index(drop=True)\n",
    "\n",
    "print(customer_feedback_df.head())\n",
    "customer_feedback_df.to_sql(name='customer_feedback', con=engine, if_exists='append', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "02370ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   transaction_id  payment_amount      payment_date  payment_method\n",
      "0       123000012         1215.87    2023/6/11 4:52  Mobile Payment\n",
      "1       123000064          474.87    2023/9/17 0:01  Mobile Payment\n",
      "2       123000121         2032.23  2023/12/25 16:16  Digital Wallet\n",
      "3       123000136           55.97   2023/7/23 15:49     Credit Card\n",
      "4       123000161          982.86  2023/11/27 21:38  Mobile Payment\n"
     ]
    }
   ],
   "source": [
    "# insert into payment\n",
    "\n",
    "df6['unit_price'] = df6['unit_price'].astype(float)\n",
    "\n",
    "# Calculate payment amount for each transaction_id\n",
    "payment_df = (\n",
    "    df6.assign(payment_amount=df6['unit_price'] * df6['quantity'])  # Calculate unit_price * quantity\n",
    "    .groupby('transaction_id', as_index=False)  # Group by transaction_id\n",
    "    .agg({\n",
    "        'payment_amount': 'sum',  # Sum the payment amounts for each transaction_id\n",
    "        'payment_date': 'first',  # Keep the first payment_date for each transaction_id\n",
    "        'payment_method': 'first'  # Keep the first payment_method for each transaction_id\n",
    "    })\n",
    ")\n",
    "\n",
    "# Ensure payment_amount is rounded to 2 decimal places\n",
    "payment_df['payment_amount'] = payment_df['payment_amount'].round(2)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(payment_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "529984ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payment_df.to_sql(name='payment', con=engine, if_exists='append', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
